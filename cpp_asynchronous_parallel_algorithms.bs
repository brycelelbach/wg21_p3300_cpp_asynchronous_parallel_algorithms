<pre class='metadata'>
Title: C++ Asynchronous Parallel Algorithms
Shortname: D3300
Revision: 1
Status: D
Group: WG21
Audience: WG21
Editor: Bryce Adelstein Lelbach (he/him/his), NVIDIA, brycelelbach@gmail.com
URL: https://wg21.link/P3300
!Source: <a href="https://github.com/brycelelbach/wg21_p3300_cpp_asynchronous_parallel_algorithms/blob/main/cpp_asynchronous_parallel_algorithms.bs">GitHub</a>
Issue Tracking: GitHub https://github.com/brycelelbach/wg21_p3300_cpp_asynchronous_parallel_algorithms/issues
Metadata Order: Editor, This Version, Source, Issue Tracking, Project, Audience
Markup Shorthands: markdown yes
Toggle Diffs: no
No Abstract: yes
Boilerplate: style-syntax-highlighting off
Default Biblio Display: direct
</pre>

# Introduction # {#introduction}

This paper outlines a unified design for asynchronous parallel algorithms and
  scheduler-aware synchronous parallel algorithms for Standard C++.

In C++17, we introduced parallel versions of the standard algorithms, which
  take an execution policy that describes what form of parallelism is allowed, if
  any.
These execution policy parallel algorithms are powerful, but have two major
  limitations:
- They are synchronous; they launch the parallel work, and then block the
    calling execution agent until the work completes.
- There is no way to specify where parallel work is executed.
- There is no way to pass tuning knobs and other parameters for execution.

In C++26, we plan to introduce [[P2300R7|senders and schedulers]], C++'s framework for
  asynchrony and controlling execution.
With senders and schedulers, we can address both of the shortcomings of the
  execution policy parallel algorithms.
We can add new asynchronous parallel algorithms that can be composed together
  and run on any scheduler.
We can also add new versions of the synchronous parallel algorithms whose
  execution can be controlled by schedulers.

# Terminology # {#terminology}

<dfn export=true><b>serial algorithm</b></dfn><br>
A version of an algorithm that does not take or return an execution policy, sender, or scheduler. E.g. All of the standard algorithms prior to C++17.

<dfn export=true><b>parallel algorithm</b></dfn><br>
A version of an algorithm that either returns a sender or takes an execution policy, sender, or scheduler. It does not necessarily execute in parallel.

<dfn export=true><b>synchronous parallel algorithm</b></dfn><br>
A parallel algorithm that blocks the calling execution agent until the parallel work that it has launched completes.

<dfn export=true><b>synchronous unscheduled parallel algorithm</b></dfn><br>
A synchronous parallel algorithm that takes an execution policy and no sender or scheduler. Therefore, users cannot control where it executes. E.g. the C++17 parallel algorithms.

<dfn export=true><b>synchronous scheduled parallel algorithm</b></dfn><br>
A synchronous parallel algorithm that takes an execution policy and a sender or scheduler. Therefore, users can control where it executes.

<dfn export=true><b>asynchronous parallel algorithm</b></dfn><br>
A parallel algorithm that returns a sender and does not block pending the completion of the parallel work it launched.

<dfn export=true><b>predecessor sender</b></dfn><br>
The single sender parameter of a synchronous scheduled parallel algorithm or an asynchronous parallel algorithm.

# Design # {#design}

We will introduce two new types of parallel algorithms: synchronous scheduled
  and asynchronous.

```
algorithm(a, b, c, d); // Serial
algorithm(pol, a, b, c, d); // Synchronous unscheduled parallel (C++17)
T t = algorithm(sch, a, b, c, d); // Synchronous scheduled parallel (New)
sender_of<T> auto s = async::algorithm(snd, a, b, c, d); // Asynchronous (New)
```

## Synchronous Scheduled Parallel Algorithms ## {#design-synchronous-scheduled-parallel-algorithms}

Synchronous scheduled parallel algorithms take a `scheduler`, from which they
  will obtain a sender by calling `schedule`.
They will launch their work using that sender, and then make exactly one call
  to `sync_wait` to block pending the completion of the work they create, to
  ensure that they block in a manner that can be customized by scheduler and
  sender authors.

## Asynchronous Parallel Algorithms ## {#design-asynchronous-parallel-algorithms}

Asynchronous parallel algorithms are sender adaptors - they take a sender as
  their first parameter and return a sender.

The returned sender will send the result you would get by calling the serial
  algorithm if it returns non-`void`, and nothing otherwise.

```
sender_of<T> auto s0 = async::algorithm_q(schedule(sch), a, b, c, d);

sender_of<void> auto s1 = async::algorithm_r(schedule(sch), e, f);

sender_of<int, bool> auto s1 = just(17, true);
sender_of<T> auto s2 = async::algorithm_q(s1, a, b, c, d);
```

Asynchronous parallel algorithms are pipeable.

```
auto s0 =
    schedule(sch)                      // `sender_of<void>`
  | async::algorithm_x(a, b, c, d)     // `sender_of<T>`, algorithm's result is `T`
  | then([] (T t) -> void { /* … */ }) // `sender_of<void>`
  | async::algorithm_y(b, e, f);       // `sender_of<U>`, algorithm's result is `U`

auto s1 =
    just(17, true)                     // `sender_of<int, bool>`
  | async::algorithm_x(a, b, c, d)     // `sender_of<T>`, algorithm's result is `T`
  | then([] (T t, int i, bool b) -> V  // `sender_of<V>`
         { /* … */ })
  | async::algorithm_y(b, e, f);       // `sender_of<U>`, algorithm's result is `U`
```

The senders returned by asynchronous parallel algorithms are awaitable, so we
  can easily use them in coroutines.

```
R serial {
  return f(algorithm_x(a, b, c, d));
}

sender_of<R> auto pipes() {
  return just() | async::algorithm_x(a, b, c, d) | then(f);
}

sender_of<R> auto coroutines() {
  co_return f(co_await async::algorithm_x(just(), a, b, c, d));
}
```

The asynchronous parallel algorithms will be based on the more modern
  `std::ranges` algorithms.
Because they differ in return type and in semantics, all asynchronous parallel
  algorithm overloads will live in a new `std::async` namespace.
The synchronous scheduled parallel algorithms will provide both legacy
  iterator-based overloads in `std` (next to their existing counterparts that
  take execution policies) and new `std::ranges` overloads.
For completeness, execution policy overloads of `std::ranges` algorithms
  should be added as well.
A separate paper on the related but orthogonal topic of parallelizing
  `std::ranges` algorithms and range adaptors is forthcoming.

## Asynchronous Parameters ## {#design-asynchronous-parameters}

There are two types of two types of dependencies that we may want to express
  when composing asynchronous algorithms.

- Data dependencies: The output of asynchronous operation `X` (a sender) may
  be needed as the input to a subsequent asynchronous operation `Y`. We want to
  be able to express this dependency without relying upon dynamic asynchronous
  mechanisms like `let_value`, which is discussed more in
  [[#issues-dynamic-asynchrony]].
- Execution-only dependencies: Sometimes we need operation `X` to happen after
  operation `Y`, even though the output of `X` will not be an input to `Y`. For
  example, two `for_each` loops that need to be performed one after the other.

The first parameter of an asynchronous parallel algorithm is the predecessor
  sender.
The predecessor sender is an execution-only dependency, representing prior work
  that must complete before the algorithm is evaluated.
It is solely an execution dependency - the algorithm does not use any values it
  sends.
The asynchronous parallel algorithm will perform its work on the predecessor
  sender's scheduler.

We explored passing inputs to the algorithm through the
  predecessor sender in [[#alternatives-single-asynchronous-parameter]], but
  decided against that approach.
We also explored allowing any of the parameters to be senders in
  [[#alternatives-dataflow-asynchronous-parameters]],
  but we also discarded that approach.

We settled on a model where we can only express execution dependencies directly.
This avoids complexity and covers many of the simple use cases elegantly.

## Properties of Execution ## {#design-properties-of-execution}

Senders and receivers have queryable properties retrievable via `get_env`.
Synchronous scheduled and asynchronous parallel algorithm receive properties
  of execution through this mechanism, including execution policy (which is
  required), allocators, stop tokens, tuning knobs, etc.
This is a subtle but key difference from the original [[P2500R2]] proposal;
  properties such as the execution policy are attached to the senders produced
  from a scheduler, not the scheduler itself.

An execution policy may be attached to a scheduler with
  `execute_with(sch, pol)` (from [[P2500R2]]), a facility which takes a
  scheduler and returns a new scheduler which will produce senders that have
  the specified execution policy attached.
A scheduler may be attached to a sender with `snd | transfer(sch)` or
  `on(sch, snd)`.
An execution policy may be attached to a sender with
  `snd | attach_execution_policy(pol)`, a new sender adaptor.
Both may be attached at the same time with `snd = schedule(sch, pol)`, `snd |
  transfer(sch, pol)`, or `on(sch, pol, snd)`, all of which are new forms of
  existing sender adaptors.

If the execution policy attached cannot be satisfied by the scheduler, a
  compilation error occurs.
If no execution policy is attached and the scheduler has no default, a
  compilation error occurs.
If the sender passed to a synchronous scheduled parallel algorithm has no
  scheduler attached to it, a compilation error occurs.

## Compositional Utilities ## {#design-compositional-utilities}

`select` and `also` are sender adaptors that aid composition of asynchronous
  parallel algorithms.
`select<N, M, ...>(snd)` chooses which values from a sender should be sent and
  in what order.
`also(snd, f)` invokes `f` with the values sent by `snd`, and then sends those
  same values, along with the result of the invocation.

```
template <std::size_t... S>
sender auto
select(sender_of<Args> auto args, F f);
// (args), (S[0], S[1], …), f -> f(args[S[0]], args[S[1]], args[…], …)

sender_of<A, B, …, invoke_result_t<F, A, B, …>> auto
also(sender_of<A, B, …> auto, F f);
// (a, b, …), f -> (a, b, …, F(a, b, …))
```

`send_values(t...)` is a function that returns an unspecified tuple-like type
  that indicates to `then` and similar adaptors that `t...` should be sent as
  values by the sender returned from the sender adaptor.
Likewise, `send_error(e)` indicates to `then` and similar adaptors that `e`
  should be sent as an error by the returned sender.

## `*_into` Algorithms ## {#design-into-reduction-algorithms}

Many reduction standard algorithms (`reduce`, `max_element`, etc) return their
  result from the algorithm.
Without a way to pass asynchronous dependencies, we found it unwieldy to compose
  simple patterns involving such algorithms.

```
iterator auto max_element(range auto rng, auto comparator);

void max_element_into(range auto rng, T& output, auto comparator);
```

In popular parallel libraries such as [CUB](https://nvidia.github.io/cccl/cub/index.html),
  reduction algorithms do not return their result, but instead take either a
  reference or an iterator to a location to write the output.
In synchronous or fire-or-forget interfaces, this allows you to avoid a memory
  transfer back to the caller, instead specifying where the result should live.
This approach can also be used for shape-changing algorithms such as `copy_if`.

When using this pattern, we found it much easier to express the code we wanted.
We propose adding such variants of at least the reduction algorithms to ease
  asynchronous composition.

# Examples # {#examples}

## Basics ## {#examples-basics}

```
auto fgh = just() | for_each(v, f) | for_each(v, g) | for_each(v, h);
```

```
// Range view chaining.
auto rolling_max = rng | slide(N) | transform(max_element);

// Asynchronous parallel algorithm chaining.
auto unique_sort = just() | async::sort(v) | async::unique(v);
```

## Normalize ## {#examples-normalize}

To normalize a range of numbers, we divide each element of a range by the
  maximum element in the range.
For example, if we have `{4 2 1 2}`, the maximum is `4`, and the normalized
  range would be `{1, 1/2, 1/4, 1/2}`.

```
void normalize_serial(range auto&& v) {
  T mx{};
  max_element_into(v, mx);
  transform(v, views::repeat(mx), begin(v), divides{});
}
```

```
void normalize_parallel(range auto&& v) {
  T mx{};
  max_element_into(par, v, mx);
  transform(par, v, views::repeat(mx), begin(v), divides{});
}
```

```
sender auto normalize_async(sender auto&& pred, range auto&& v) {
  auto tmp = pred | then([] { return T{}; });
  return let_value(tmp, [] (T&& mx) {
      return async::max_element_into(just(), v, mx)
           | async::transform(v, views::repeat(mx), begin(v), divides{});
    });
}
```

## Maximum Gap ## {#examples-maximum-gap}

Find the maximum difference between two successive elements in the sorted form
  of the range.
This is a well-known [LeetCode problem](https://leetcode.com/problems/maximum-gap).
For example, the maximum gap of `{3, 6, 9, 1}` is `3`.

```
T maximum_gap_serial(range auto&& v) {
  vector<T> buf(v);
  T mx{};
  sort(buf);
  adjacent_difference(buf, begin(buf));
  max_element_into(buf | drop(1), mx);
  return mx;
}
```

```
T maximum_gap_parallel(range auto&& v) {
  vector<T> buf(v);
  T mx{};
  sort(par, buf);
  adjacent_difference(par, buf, begin(buf));
  max_element_into(par, buf | drop(1), mx);
  return mx;
}
```

```
sender auto maximum_gap_async(sender auto&& pred, range auto&& v) {
  auto tmp = pred | then([&] { return send_values(vector<T>(v), T{}); });
  return let_value(tmp, [] (vector<T>&& buf, T&& mx) {
      return async::sort(just(), tmp)
           | async::adjacent_difference(buf, begin(buf))
           | async::max_element_into(buf | drop(1), mx)
           | then([] { return mx; });
    });
}
```

## Rain Water ## {#examples-rain-water}

Given a range of non-negative numbers representing an elevation map where the
  width of each bar is 1, compute how much water can be trapped after raining.
This is a well-known [LeetCode problem](https://leetcode.com/problems/trapping-rain-water).
For example, if we have `{1, 0, 2, 0, 2}`, then 3 units of water can be trapped.

```
int rain_water_serial(range auto&& v) {
  vector<int> buf(size(v));
  int mx{};
  int sum{};
  max_element_into(v, mx);
  inclusive_scan(begin(v),  next(it), begin(buf), ranges::max);
  inclusive_scan(rbegin(v), reverse_iterator(it),  rbegin(buf), ranges::max);
  transform_reduce_into(buf, cbegin(v), sum, 0, plus{}, minus{});
  return sum;
}
```

```
int rain_water_parallel(range auto&& v) {
  vector<int> buf(size(v));
  int mx{};
  int sum{};
  max_element_into(par, v, mx);
  inclusive_scan(par, begin(v),  next(it), begin(buf), ranges::max);
  inclusive_scan(par, rbegin(v), reverse_iterator(it),  rbegin(buf), ranges::max);
  transform_reduce_into(par, buf, cbegin(v), sum, 0, plus{}, minus{});
  return sum;
}
```

```
sender auto rain_water_async(range auto&& v) {
  auto tmp = pred | then([&] {
    return send_values(vector<int>(size(v)), int{}, int{});
  });
  return let_value(tmp, [] (vector<int>&& buf, int&& mx, int&& sum) {
      return async::max_element_into(just(), v, mx)
           | async::inclusive_scan(begin(v),  next(it), begin(buf), ranges::max)
           | async::inclusive_scan(rbegin(v), reverse_iterator(it),  rbegin(buf), ranges::max)
           | async::transform_reduce_into(buf, cbegin(v), sum, 0, plus{}, minus{})
           | then([] { return sum });
    });
}
```

## Upper Triangular Cholesky Factorization ## {#examples-upper-triangular-cholesky-factorization}

```
void upper_triangular_cholesky_factorization_parallel(
  la_matrix A, la_vector b, la_vector x)
{
  // Solve U^T c = b, using x to store c
  triangular_matrix_vector_solve(
    par,
    transposed(A), upper_triangle, explicit_diagonal,
    b,
    x,
    divides{});

  // Solve U x = c, overwriting x with result
  triangular_matrix_vector_solve(
    par,
    A, upper_triangle, explicit_diagonal,
    x,
    divides{});
}
```

```
sender auto upper_triangular_cholesky_factorization_async(
  sender auto&& pred, la_matrix A, la_vector b, la_vector x)
{
  return pred
         // Solve U^T c = b, using x to store c
       | async::triangular_matrix_vector_solve(
           transposed(A), upper_triangle, explicit_diagonal,
           b,
           x,
           divides{});
         // Solve U x = c, overwriting x with result
       | async::triangular_matrix_vector_solve(
           A, upper_triangle, explicit_diagonal,
           x,
           divides{});
}
```

# Alternatives # {#alternatives}

## Asynchronous Parameters ## {#alternatives-asynchronous-parameters}

### Dataflow Asynchronous Parameters ### {#alternatives-dataflow-asynchronous-parameters}

We explored the idea of allowing any parameter of an asynchronous parallel
  algorithm to be a sender.
In serial algorithms, each of these parameters is immediately available,
  meaning they are a non-sender object.
These parameters are either ranges, iterators, scalars, or operations.

In this proposed design, each of these parameters could be either an
  immediately available object or a sender of the same type of object.
This design is inspired by [HPX's dataflow](https://hpx-docs.stellar-group.org/branches/master/html/libs/core/async_base/api/dataflow.html)
  primitive.

```
template <class T>
concept immediate_or_sender = T || sender_of<T>;
```

Asynchronous parallel algorithms would perform a `when_all` on all of the
  algorithmic parameters that are senders, and then launch their work on the
  predecessor sender's scheduler.

```
sender_of<T> async::__reduce(sender auto&& pred,
                             range auto&& rng,
                             T init,
                             Op op);

sender_of<T> async::reduce(sender auto&& pred,
                           sender_or_immediate<range> auto&& rng,
                           sender_or_immediate<T> init,
                           sender_or_immediate<Op> op)
{
  return when_all(__collect_senders(rng, init, op))
       | let_value([] (range auto&& rng, T init, Op op) {
           return pred | async::__reduce(rng, init, op);
         });
}
```

This gives us a straightforward way to express data dependencies.

```
sender auto normalize_async_dataflow(range auto&& v) {
  auto mx = async::fold_left(just(), v, ranges::max{}) | split;
  return async::transform(mx, v, mx | then(views::repeat), begin(v), divides{});
}
```

```
sender auto maximum_gap_async_dataflow(range auto&& v) {
  auto tmp = just(vector<T>(v))) | split;
  return async::sort(just(), tmp);
       | async::adjacent_difference(tmp, tmp | then(begin));
       | async::max_element(tmp | then(drop(1)));
       | then([] (auto it) { return *it; });
}
```

```
sender auto rain_water_async_dataflow(range auto&& v) {
  auto tmp = just(vector<T>(size(v))) | split;
  auto it = async::max_element(just(), v) | split;
  auto left = async::inclusive_scan(it,
    begin(v), it | then(next), tmp | then(begin), ranges::max);
  auto right = async::inclusive_scan(it,
    rbegin(v), it | then([] (auto i) { return reverse_iterator(i) }),
    tmp | then(rbegin), ranges::max);
  return async::transform_reduce(when_all(left, right),
    tmp, cbegin(v), 0, plus{}, minus{});
}
```

We did consider omitting the predecessor sender, however then it would become
  unclear where the algorithm would run, just as it can be for `when_all`.
It would also leave us unable to express execution-only dependencies in a clear
  fashion.


We ran into a few problems with this design.
In many cases it lead to senders being passed and connected multiple times.
This might happen when passing a sender to multiple different algorithm calls,
  because they need to operate on the same entity.
It could also happen if you need to pass a sender both as the predecessor (to
  indicate where completion should occur) and as an input to the algorithm.

Connecting senders multiple times is problematic in this context.
It is easy to accidentally copy a sender and duplicate computations that may be
  expensive.
In fact, this is often the default behavior if you don't go out of your way to
  avoid it.
Alternatively, you must judiciously use `split`, but this introduces dynamic
  allocation and synchronization.
Additionally, there seems to be no way to get a non-const lvalue reference to
  be sent to multiple receivers even with `split`.

### Single Asynchronous Parameter ### {#alternatives-single-asynchronous-parameter}

We explored a design where the parameters to the algorithms (ranges, iterators,
  invocables) could be provided asynchronously by the predecessor sender instead
  of being supplied at the time the asynchronous algorithm is invoked.
If the predecessor sender sends N values, they correspond to the first N
  parameters of the parallel algorithm.
None of the parameters may be skipped; they bind from the front, like
  `bind_front`.
Any parameters that are not sent by the sender must be passed after the
  predecessor parameter.
The sender may send no parameters, in which case they all must be provided
  after the predecessor sender parameter.

```
// Synchronous scheduled parallel (New)
value = algorithm(schedule(sch), a, b, c, d);
value = algorithm(transfer_just(sch, a), b, c, d);
value = algorithm(transfer_just(sch, a, b), b, c, d);
value = algorithm(transfer_just(sch, a, b, c), d);
value = algorithm(transfer_just(sch, a, b, c));

// Asynchronous parallel (New)
snd = async::algorithm(just(), a, b, c, d);
snd = async::algorithm(just(a), b, c, d);
snd = async::algorithm(just(a, b), b, c, d);
snd = async::algorithm(just(a, b, c), d);
snd = async::algorithm(just(a, b, c, d));
```

The goal of this model was composability.
We wanted to enable the same point-free style programming used by range adaptor
  pipelines, where the inputs and outputs of each operation are not explicitly
  named and flow into each other.

We found that this model worked best when you are working with a single range
  and performing operations in-place:

```
auto fgh = just(v) | for_each(f) | for_each(g) | for_each(h);

auto unique_sort = just(v) | async::sort | async::unique;
```

However, we found that the model broke down when writing more complex code.
We often had to use the `also` and `select` utilities and additionally insert
  `then` adaptors that would adjust parameters in between algorithm
  invocations - increment an iterator by one, dereference an iterator, turn a
  scalar into a range view, get an iterator from a range, etc.

```
sender_of<range> auto normalize_async(range auto&& rng) {
  return async::max_element(rng)
       | then([] (auto rng, auto mx) {
           return send_values(rng, repeat(*mx));
         })
       | inplace(async::transform(divides{}));
}

sender_of<la_vector> auto upper_triangular_cholesky_factorization_async(
  la_matrix A, la_vector b, la_vector x)
{
  return just(transposed(A), upper_triangle, explicit_diagonal, b, x)
       | also(async::triangular_matrix_vector_solve(divides))
         // Receives (transposed(A), upper_triangle, explicit_diagonal, b, x)
         // Sends (transposed(A), upper_triangle, explicit_diagonal, b, x)
       | select<0, 1, 2, 4>(then([] (auto A, auto tri, auto dia, auto x) {
           return send_values(transposed(A), tri, dia, x);
         })) // Drop b and untranspose A.
         // Sends (A, upper_triangle, explicit_diagonal, x)
       | async::triangular_matrix_vector_solve(divides{});
}
```

We were particularly plagued by mismatches between range parameters (which are
  only used for the primary input to algorithms), iterator parameters (which are
  used for auxiliary inputs and outputs to algorithms), and non-scalar return
  types (most algorithms return iterators-past-the-end, which can't be fed in as
  the next primary input, which needs to be a range).

Many algorithms are not in-place, such as `transform`.
An `inplace(snd, adaptor)` that transforms an out-of-place asynchronous
  algorithm into an in-place one helps with that:

```
sender_of<decltype(Adaptor{}(R{}, iterator_t<R{}>))> auto
inplace(sender_of<R> r, Adaptor a);
// r, a -> a(r, begin(r))

auto fgh = just(v)
         | inplace(transform(f))
         | inplace(transform(g))
         | inplace(transform(h));
```

## Logical Return Types ## {#alternatives-logical-return-types}

As part of the single asynchronous parameter design, we planned to have the
  asynchronous parallel algorithms return a sender that sends the logical output
  of the algorithm, suitable for piping into another parallel algorithm.
This asynchronous value will often differ from the return value of the
  corresponding non-parallel algorithm.
For example, `transform` returns a single iterator to the end of the
  transformed range, but `async::transform` should send the transformed range.
This is implementable because parallel algorithms require forward iterators.

<table>
<tr>
<th>
Algorithm
<th>
Non-Parallel Returns...
<th>
Asynchronous Sends...

<tr>
<td>
```
transform(Range0 rng0,
          [Range1 rng1,]
          Iterator out,
          F f)
```
<td>
Iterators to the last transformed element of `rng0` and `rng1` and an iterator
  to the element past the last transformed element of `out`.
<td>
The range of transformed elements of `out`, e.g. `subrange(out, advance(out, N))`,
  where `N` is the number of elements transformed.

<tr>
<td>
```
for_each(Range rng,
         F f)
```
<td>
An iterator to the element past the last transformed element of `rng` and the
  object `f`.
<td>
`rng`.

<tr>
<td>
```
reduce(Range rng,
       T init,
       F f)
```
<td>
The sum as a T.
<td>
The sum as a T.

<tr>
<td>
```
fold_*(Range rng,
       [T init,]
       F f)
```
<td>
The sum as a T.
<td>
The sum as a T.

<tr>
<td>
```
find_if(Range rng,
        Pred p)
```
<td>
An iterator to the found element.
<td>
An iterator to the found element.

<tr>
<td>
```
(min|max)_element(Range rng,
                  Comp c)
```
<td>
An iterator to the found element.
<td>
An iterator to the found element.

<tr>
<td>
```
copy(Range from,
     Iterator to)
```
<td>
An iterator to the last copied element of `from` and an iterator to the element
  past the last copied element of `to`.
<td>
The range of copied elements of `to`, e.g. `subrange(to, advance(to, N))`, where
  `N` is the number of elements copied.

<tr>
<td>
```
copy_if(Range from,
        Iterator to)
```
<td>
An iterator to the last copied element of `from` and an iterator to the element
  past the last copied element of `to`.
<td>
The range of copied elements of `to`, e.g. `subrange(to, advance(to, M))`,
  where `M` is the number of elements copied.

<tr>
<td>
```
*_scan(Range rng,
       Iterator out,
       T init,
       F f)
```
<td>
An iterator to the element past the last scanned element of `out` (this is what
  the non-parallel overload returns, as there's no parallel overload yet)
<td>
The range of scanned elements of `out`, e.g. `subrange(out, advance(out, N))`,
  where `N` is the number of elements scanned.

<tr>
<td>
```
sort(Range rng,
     Comp c)
```
<td>
An iterator to the last sorted element.
<td>
`rng`.

<tr>
<td>
```
unique(Range rng,
       Comp c)
```
<td>
The range of unique elements of `rng`, e.g.
  `subrange(begin(rng), advance(begin(rng), N))`, where `N` is the number of
  unique elements.
<td>
The range of unique elements of `rng`, e.g.
  `subrange(begin(rng), advance(begin(rng), N))`, where `N` is the number of
  unique elements.

<tr>
<td>
```
partition(Range rng,
          Comp c)
```
<td>
The range of the group of `rng` for which `c` is false (the second group), e.g.
  `subrange(advance(begin(rng), M), end(rng))`, where `M` is the number of
  elements for which `c` is true.
<td>
The range of the group of `rng` for which `c` is false (the second group), e.g.
  `subrange(advance(begin(rng), M), end(rng))`, where `M` is the number of
  elements for which `c` is true.

</table>

We eventually reached the conclusion that this whole approach was too complex,
  and that it would be better to use `let_value` and named variables to handle
  asynchronous parameters.
The major downside to this approach is that `let_value` requires dynamic
  parallelism, which is undesirable on some platforms.
This is discussed more in [[#issues-dynamic-asynchrony]].

## Passthrough of Values from Predecessor ## {#passthrough-of-values-from-predecessor}

Previously, we proposed that the sender returned by asynchronous algorithms
  would send the values sent by the predecessor, followed by the result you
would get by calling the serial algorithm (if it returns non-`void`).

```
sender_of<T> auto s0 = async::algorithm(schedule(sch), a, b, c, d);

sender_of<int, bool> auto s1 = just(17, true);
sender_of<int, bool, T> auto s2 = async::algorithm(s1, a, b, c, d);

auto s3 =
    schedule(sch)                      // `sender_of<void>`
  | async::algorithm_x(a, b, c, d)     // `sender_of<T>`, algorithm's result is `T`
  | then([] (T t) -> void { /* … */ }) // `sender_of<void>`
  | async::algorithm_y(b, e, f);       // `sender_of<U>`, algorithm's result is `U`

auto s4 =
    just(17, true)                     // `sender_of<int, bool>`
  | async::algorithm_x(a, b, c, d)     // `sender_of<int, bool, T>`, algorithm's result is `T`
  | then([] (T t, int i, bool b) -> V  // `sender_of<V>`
         { /* … */ })
  | async::algorithm_y(b, e, f);       // `sender_of<V, U>`, algorithm's result is `U`
```

There is precedent for this design in `bulk`.
`bulk` is a sender adaptor, does not take an explicit scheduler or execution
  policy, and passes through the values sent by its predecessor.

The advantage to this approach is that no information is discarded; you drop
  neither the predecessor values (which might be needed as inputs to subsequent
  operations) nor the result of the asynchronous algorithm.
However, this came with the significant downside of added verbosity and
  complexity, leading to pathological sender types in longer chains, such as:

```
vector<T> v(/* ... */);

sender_of<vector<T>::iterator, vector<T>::iterator, vector<T>::iterator> fgh =
    just()
  | transform(v, begin(v), f)
  | transform(v, begin(v), g)
  | transform(v, begin(v), h);
```

This pass through arose out of the single asynchronous parameter design, and
  may have made more sense in that context, or in a world where we had a
  powerful pipeline operator that enabled point-free-style composition of
  operations that take multiple inputs of varying shapes.
However, we believe that with the current design, pass through does not make
  sense.
Algorithms should simply send their result.
If the values sent by the predecessor need to be preserved, the predecessor
  should be `spilt` before passing it to the algorithm.

## Attach Execution Policies to Schedulers or Vice Versa ## {#alternatives-attach-to-execution-policeis-or-schedulers}

[[P2500R2]] suggests a different approach for synchronous scheduled parallel
  algorithms.
It proposes that we attach execution policies to schedulers, and then pass
  these policy-aware schedulers as a parameter to the synchronous scheduled
  parallel algorithms, instead of passing a sender parameter as we suggest.

We believe that attaching execution policies to schedulers or vice versa is not
  a clean design.
We already have a property-like mechanism for attaching things to
  senders/receivers (`get_env`, sender attributes, and receiver environments).
If we also allow execution policies to have things attached to them, we'll end
  up introducing an additional system which may interact poorly with the
  sender/receiver property mechanism.

Many of the sender/receiver properties are applicable to the synchronous
  scheduled parallel algorithms too.
How do we pass an allocator to a synchronous parallel algorithm?
How do we pass a stop token to a synchronous scheduled parallel algorithm?
Do we attach those to schedulers as well?
We'll end up needing every sender/receiver property on schedulers as well.

If both senders/receivers and schedulers can have the same kinds of things
  attached to them, and schedulers can be attached to senders/receivers, we can
  end up with collisions and confusion.
Should an asynchronous parallel algorithm use the execution policy attached to
  the sender/receiver?
Or should it use the execution policy attached to the scheduler that's attached
  to the sender/receiver?
What if both exist and disagree?

We have ample experience with property systems on execution policies in the
  Thrust library, which was the inspiration for C++'s parallel algorithms and
  has over 15 years of field experience and hundreds of thousands of users.
Initially, we introduced one property for execution policies, but over time,
  user demand for tuning knobs and controls led to the addition of others.
Today, Thrust's execution policies can have the moral equivalent of schedulers
  (streams), allocators, and asynchronous dependencies attached to them.
The interactions between these different properties and their lifetime model is
  brittle and difficult to understand.
Our field experience with this model has largely been negative.

It is much simpler for us to have one property system (the existing one for
  senders/receivers), attach both schedulers and execution policies via that
  property system, and then parameterize both the synchronous and asynchronous
  parallel algorithms on senders, not execution policies.

## Require Explicit Execution Policy and/or Scheduler Parameters ## {#alternatives-explicit-properties}

Another alternative design would be to require explicit execution policy and/or
  scheduler parameters on every parallel algorithm invocation.
This might be reasonable, albeit verbose, for the synchronous scheduled
  parallel algorithms.
For the asynchronous parallel algorithms, this would significantly constrain
  their functionality and make it harder to write generic code.

You may not know which execution policy or scheduler should be used at the time
  that you are calling an asynchronous parallel algorithm; that's fine, because
  they can be attached later
If you do know that a particular execution policy or scheduler is needed,
  that's also fine, because you can attach them early.

```
// Early attaching: We know the types and operations and can decide what to use.
auto unique_sort(vector<int>& v) {
  return schedule(system_scheduler, par) | sort(v) | unique(v);
}

// Late attaching: We don't know all the types and operations.
auto pattern(range auto&& v, auto f, auto g, auto h) {
  return just() | async::transform(v, f) | async::then(g) | async::transform(v, h);
}

// The decision about execution policy and scheduler to use should be local to
// the knowledge of the types and operations.
auto s = on(sch, pol, pattern(d, x, y, z));
```

We do not require that users specify a scheduler for every sender adaptor
  today; we should not require this of asynchronous parallel algorithms either.

# Issues # {#issues}

## Dynamic Asynchrony ## {#issues-dynamic-asynchrony}

Asynchrony is dynamic when asynchronous work is enqueued by other asynchronous
  work.
In some parallel programming environments, dynamic asynchrony is either
  expensive or impossible to implement.

This is a common issue for GPU programming models like CUDA - launching new
  asynchronous work from the GPU is either expensive or impossible.
For CUDA in particular, we have a feature called CUDA Dynamic Parallelism (CDP)
  that allows us to launch new GPU work from the GPU.
The original implementation of CDP had such significant overheads that it was
  almost never used.
CUDA recently released a new CDP model that offers much better overheads - but
  the launch latency is still approximately 2x worse than CPU launch latency.

Most of the senders framework can be implemented without dynamic asynchrony.
However, that is not the case for `let_value`, which is inherently dynamic, as
  we cannot know what the next sender will be until we connect and invoke the
  user-provided invocable.
In our current implementation of CUDA schedulers, `let_value` will block until
  the entire predecessor chain completes on the CPU side, introducing a
  substantial latency bubble that destroys performance.
We believe a better implementation is possible using the new CUDA CDP model,
  however this will still have substantial overheads that are best avoided.

For this reason, the NVIDIA mindset on `let_value` has typically been that it
  is to be avoided or used sparingly, as it will have performance issues on our
  platform.
This is one of the reasons we pursued a point-free-style design, as it was
  hoped this would limit the need for `let_value`.
However, we do not see a path for an elegant point-free-style design, so we are
  now resigned to using `let_value`, despite its performance pitfalls.

## Lifetimes ## {#issues-lifetimes}

While most of the serial algorithms take arguments by value, the range-based
  ones tend to take ranges by universal reference.
That's fine because they complete immediately.
But it may be a bit of a footgun for asynchronous parallel algorithms, just as
  it is for range views.

<style>
table, th, tr, td {
  border: 2px solid black !important;
}
@media (prefers-color-scheme: dark) {
  table, th, tr, td {
    border: 2px solid white !important;
  }
}
</style>

# Changelog # {#changelog}

## R1 ## {#changelog-r1}

- Add section exploring HPX-dataflow-inspired asynchronous parameters.
- Change examples to take a predecessor sender.
- Switch to `*_into` variant for reductions.
- Change predecessor sender value passthrough to append instead of prepending.
- Remove passthrough of values from the predecessor from the proposed design,
    but leave it in the alternatives section.
- Change synchronous scheduled algorithms to take schedulers, not senders, as
    [[P2500R2]] proposes.
- Rename the compositional utility `with` to `also`.
- Add fixes and more explanation of examples.

